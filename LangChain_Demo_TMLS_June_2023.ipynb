{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prinella-cyber/FirstGit/blob/main/LangChain_Demo_TMLS_June_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXAV-SMwk4px"
      },
      "source": [
        "# Chatting with our friend's plant based recipes\n",
        "\n",
        "**Context**\n",
        "\n",
        "- You've been inspired by a friend, and have decided to try eating plant based for a week.\n",
        "- You really like experimenting in the kitchen, and want to try making everything from scratch this week. No packaged foods!\n",
        "- You're not sure where to start. Normally you could ask your friend, but they're away right now so they can't help you.\n",
        "- Your friend sends a text file of all their plant based recipes to you.\n",
        "\n",
        "Download the text file here : https://drive.google.com/file/d/1xILOymt2HV-zxHd0Zft5olviH1btJEe2/view?usp=sharing\n",
        "\n",
        "**The Challenge**\n",
        "\n",
        "You want to use LangChain to build a chat application to:\n",
        "1. Teach you how to cook plant based\n",
        "2. Efficiently explore your friend's recipes.\n",
        "\n",
        "**The Method**\n",
        "\n",
        "You want to learn the following things:\n",
        "\n",
        "1. Given an ingredient, what is the best plant based substitute?\n",
        "- e.g., given milk, the best plant based substitute is almond milk.\n",
        "2. Given that best plant based substitute, how can you actually make it at home? (Because you want to make everything from scratch!)\n",
        "- e.g., given the plant base substitue almond milk, you can make it by blending together 1/4 cup of almonds with 1 cup of water.\n",
        "3. Does your friend have any recipes that use this substitute?\n",
        "- e.g., do any of the recipes in the file your friend sent you contain almond milk?\n",
        "\n",
        "**Let's go!**\n",
        "\n",
        "Let's see how we can use LangChain to solve this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR1MOa-VwHLv"
      },
      "source": [
        "# 1. Given an ingredient, what is the best plant based substitute?\n",
        "\n",
        "We don't want to know how to make it yet. We just want to know what the best substitute is. We don't want to overwhelm ourselves with information because we're new to this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0RtczrWn7U9"
      },
      "source": [
        "## Set up: Installation, imports, and API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HUWx0_JnF2N"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXgjzP0_154N"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.26"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utyhh7Vnm8TJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "OPENAI_API_KEY = 'YOUR-OPEN-AI-API-KEY-HERE'\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KjBNX9Rm_0e"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SjtQJCMoA6U"
      },
      "source": [
        "## Getting familiar with the building blocks of LangChain\n",
        "\n",
        "Remember what the building blocks are? PROMPTS and CHAINS.\n",
        "\n",
        "### Prompts\n",
        "\n",
        "What is a prompt? It's a way to talk to a language model. Prompts are a way to query a language model repeatedly for the same purpose.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atP7M6d4nBX2"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain import LLMChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdk453UJIUpJ"
      },
      "source": [
        "We know we are going to, potentially, want to know again and again the best plant based substitutes for an ingredient.\n",
        "\n",
        "We could write \"What's the best substitute for XYZ?\" over and over again. But with a prompt template, we can easily just parameterize in that 'XYZ'.\n",
        "\n",
        "From the LangChain docs:\n",
        "\n",
        "*A prompt template refers to a reproducible way to generate a prompt. It contains a text string (“the template”), that can take in a set of parameters from the end user and generate a prompt.*\n",
        "\n",
        "*The prompt template may contain:*\n",
        "\n",
        "*- instructions to the language model,*\n",
        "\n",
        "*- a set of few shot examples to help the language model generate a better response,*\n",
        "\n",
        "*- a question to the language model.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXMhQ7g35EYG"
      },
      "outputs": [],
      "source": [
        "# initialize our llm\n",
        "llm = OpenAI(\n",
        "    temperature=0.9,\n",
        ")\n",
        "\n",
        "# create prompt\n",
        "# formatted for ease of readability, but not necessary\n",
        "# add instructions to say \"don't tell me how\", and \"use whole foods\".\n",
        "# Note that we don't f-string it in.\n",
        "# Could also say \"Don't give me multiple responses. Just one ingredient will do.\"\n",
        "vegan_ingredient_template = \"\"\"\n",
        "  Convert the non-vegan ingredient to the single best vegan version.\n",
        "  Don't tell me how to make it. Just tell me the ingredient.\n",
        "  Use whole foods.\n",
        "\n",
        "  Ingredient: {ingredient}\n",
        "\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "# pass into PromptTemplate\n",
        "vegan_ingredient_prompt = PromptTemplate(\n",
        "    input_variables=[\"ingredient\"],\n",
        "    template=vegan_ingredient_template,\n",
        ")\n",
        "# and then pass the prompt into our first chain - we will talk more about chains later\n",
        "vegan_ingredient = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=vegan_ingredient_prompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ96a1Kpxm-t"
      },
      "source": [
        "Great, so our first prompt has been created. Let's try asking the LLM some questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZJnkLEMxmhH",
        "outputId": "ce356e60-b2d0-48ce-b16c-fc3848648caa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsweetened almond milk\n"
          ]
        }
      ],
      "source": [
        "ingredient = \"2 to 2.5 tablespoon milk\"\n",
        "print(vegan_ingredient.run(ingredient))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUqhnAYbxv2z"
      },
      "source": [
        "Let's try a few more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieu6aT0oIwRH",
        "outputId": "32e2b05f-6ef3-4f25-9fa6-28d1e4398164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 cup coconut oil\n"
          ]
        }
      ],
      "source": [
        "ingredient = \"1 cup butter\"\n",
        "print(vegan_ingredient.run(ingredient))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npdXYnMiIzXR",
        "outputId": "3523f976-e134-4705-f925-b23f2b17bc4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Soy crumbles\n"
          ]
        }
      ],
      "source": [
        "ingredient = \"1 pound ground beef\"\n",
        "print(vegan_ingredient.run(ingredient))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg38T4iZx35A"
      },
      "source": [
        "Ok great, so at this point we understand a bit more about what the best plant based substitutes are for our ingredients.\n",
        "\n",
        "And along the way, we have learned how to create and use a simple prompt template.\n",
        "\n",
        "Next up: let's try actually learning how to make these substitutes at home!\n",
        "\n",
        "# 2. Given that best plant based substitute, how can you actually make it at home? (Because you want to make everything from scratch!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh8qVMRUyPe3"
      },
      "source": [
        "As before, let's create a prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmDc75nUI4uM"
      },
      "outputs": [],
      "source": [
        "vegan_instructions_template = \"\"\"How do I make this ingredient?\n",
        "\n",
        "Ingredient: {ingredient}\n",
        "Instructions: \"\"\"\n",
        "\n",
        "vegan_instructions_prompt = PromptTemplate(\n",
        "    input_variables=[\"ingredient\"],\n",
        "    template=vegan_instructions_template,\n",
        ")\n",
        "\n",
        "vegan_instructions = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=vegan_instructions_prompt\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDtJBcCCywDF"
      },
      "source": [
        "Great, like before, we've created a simple prompt. Let's try running it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6HkKHYVyvtW",
        "outputId": "400109b0-3065-4383-9146-bcb154c8f1a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Combine 1 cup almonds with 4 cups of water in a high-speed blender.\n",
            "\n",
            "2. Blend for 1-2 minutes until the almonds are fully broken down and the mixture is creamy.\n",
            "\n",
            "3. Strain the mixture through a nut milk bag or cheesecloth, discarding the almond pulp.\n",
            "\n",
            "4. Pour the strained almond milk into a mason jar or airtight container and store in the fridge for up to 4 days. Enjoy!\n"
          ]
        }
      ],
      "source": [
        "ingredient = 'Unsweetened almond milk'\n",
        "print(vegan_instructions.run(ingredient))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE9_lNYyyy8Z"
      },
      "source": [
        "Hm but actually, you're feeling a bit particular. You want the results to be just in a sentence.\n",
        "\n",
        "So you give the LLM some instructions on how to construct the output. This is few shot learning!\n",
        "\n",
        "With LangChain, it's very easy to do this through the use of prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny_Jhhv9GuUy",
        "outputId": "26dac92e-12a8-4287-d9ac-ffa456cfcd7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Blend together equal parts almonds and filtered water, then strain.\n"
          ]
        }
      ],
      "source": [
        "vegan_method_template = \"\"\"How do I make this ingredient?\n",
        "\n",
        "Don't create a list. Just output the instructions in a single sentence.\n",
        "\n",
        "Example 1:\n",
        "- Ingredient: 'tofu steak'\n",
        "- The question you understand this as is: 'how do I make tofu steak'?\n",
        "- Instructions: 'Marinade extra firm tofu for 15 minutes in oil and spices and then grill for 10 minutes'.\n",
        "\n",
        "Example 2:\n",
        "- Ingredient: 'tofu aioli'\n",
        "- The question you understand this as is: 'how do I make tofu aioli'?\n",
        "- Instructions: 'Blend together tofu, lemon juice, salt, and pepper.'\n",
        "\n",
        "Ingredient: {ingredient}\n",
        "Instructions: \"\"\"\n",
        "\n",
        "vegan_method_prompt = PromptTemplate(\n",
        "    input_variables=[\"ingredient\"],\n",
        "    template=vegan_method_template,\n",
        ")\n",
        "\n",
        "vegan_method = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=vegan_method_prompt,\n",
        ")\n",
        "\n",
        "ingredient = 'unsweetened almond milk'\n",
        "print(vegan_method.run(ingredient))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRltCgqnJW9G"
      },
      "source": [
        "That looks better!\n",
        "\n",
        "But wait, what I did was essentially I did 2 calls to the language model:\n",
        "\n",
        "1. get the vegan version of an ingredient, by running `vegan_ingredient.run(ingredient)` --> I passed in `milk`, and the LLM gave us `unsweetened almond milk'`.\n",
        "2. get the method for how to make that ingredient, by running `vegan_method.run(ingredient)`, where I passed in `unsweetened almond milk`.\n",
        "\n",
        "I copied the output of step 1 and used it as an input to step 2.\n",
        "\n",
        "So I made two calls to an LLM, where the output of the first is the input to the second.\n",
        "\n",
        "There's a chain for that in LangChain! This is what I mean when I say LangChain helps make developing LLM applications easier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0c7iXx6oUFI"
      },
      "source": [
        "## Chains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPd_FavMJamm"
      },
      "source": [
        "We're going to CHAIN the calls to the LLM together.\n",
        "\n",
        "From the LangChain docs:\n",
        "\n",
        "*Chains allow us to combine multiple components together to create a single, coherent application.*\n",
        "\n",
        "*For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM*.\n",
        "\n",
        "This is exactly what we would like to do!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAqYHsFwC51A"
      },
      "source": [
        "So we want to make two calls to a language model. We can do this using a SequentialChain.\n",
        "\n",
        "In this series of chains, each individual chain has a single input and a single output, and the output of one step is used as input to the next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQcOcE-qC4R5"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "# instead of making 2 calls to our chains and manually passing in the output of `vegan_ingredient`\n",
        "# as an input to `vegan_method`, we can just directly use a SimpleSequentialChain.\n",
        "# SimpleSequentialChain will take care of passing the output of `vegan_ingredient` into `vegan_method` for us!\n",
        "vegan_ingredient_and_method_chain = SimpleSequentialChain(\n",
        "    chains=[\n",
        "        vegan_ingredient,\n",
        "        vegan_method,\n",
        "    ],\n",
        "    verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFbhA09r0OG0"
      },
      "source": [
        "And that's it ! It's that simple. We have defined our chains, `vegan_ingredient`, and `vegan_method` already.\n",
        "\n",
        "Now let's call our SimpleSequentialChain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vZzVrLX0mVm",
        "outputId": "7a069c2a-e46b-4177-889b-f3b0e153345b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m  Plant-based milk (such as almond, oat, coconut, soy, etc.)\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m Blend together water and plant-based milk of your choice in 1:1.5 ratio.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "memory=None callbacks=None callback_manager=None verbose=True chains=[LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, prompt=PromptTemplate(input_variables=['ingredient'], output_parser=None, partial_variables={}, template=\"\\n  Convert the non-vegan ingredient to the single best vegan version.\\n  Don't tell me how to make it. Just tell me the ingredient.\\n  Use whole foods.\\n  Don't give me multiple responses. Just one ingredient will do.\\n\\n  Ingredient: {ingredient}\\n\\n  Answer:\\n\", template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.9, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all'), output_key='text'), LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, prompt=PromptTemplate(input_variables=['ingredient'], output_parser=None, partial_variables={}, template=\"How do I make this ingredient?\\n\\nDon't create a list. Just output the instructions in a single sentence.\\n\\nExample 1:\\n- Ingredient: 'tofu steak'\\n- The question you understand this as is: 'how do I make tofu steak'?\\n- Instructions: 'Marinade extra firm tofu for 15 minutes in oil and spices and then grill for 10 minutes'.\\n\\nExample 2:\\n- Ingredient: 'tofu aioli'\\n- The question you understand this as is: 'how do I make tofu aioli'?\\n- Instructions: 'Blend together tofu, lemon juice, salt, and pepper.'\\n\\nIngredient: {ingredient}\\nInstructions: \", template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.9, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all'), output_key='text')] strip_outputs=False input_key='input' output_key='output'\n"
          ]
        }
      ],
      "source": [
        "# Run the chain specifying only the input variable for the first chain.\n",
        "vegan_ingredient_method = vegan_ingredient_and_method_chain.run(\n",
        "    \"milk\"\n",
        "    )\n",
        "print(vegan_ingredient_and_method_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFwhYS2vJ8Ej",
        "outputId": "4e88a10c-8fa0-46aa-cf41-30f042f13361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m  1 pound of crumbled tempeh.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m Marinate the crumbled tempeh in a mixture of oil, soy sauce, garlic, and spices for at least 30 minutes before cooking.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Marinate the crumbled tempeh in a mixture of oil, soy sauce, garlic, and spices for at least 30 minutes before cooking.\n"
          ]
        }
      ],
      "source": [
        "vegan_ingredient_method = vegan_ingredient_and_method_chain.run(\n",
        "    \"1 pound ground beef\"\n",
        "    )\n",
        "print(vegan_ingredient_method)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q_K0aNdKBYu"
      },
      "source": [
        "It's looking good! Let's revisit what we were aiming to achieve:\n",
        "\n",
        "1. Given an ingredient, what is the best plant based substitute? **DONE!**\n",
        "2. Given that best plant based substitute, how can you actually make it at home? (Because you want to make everything from scratch!) **DONE!**\n",
        "\n",
        "But we still need to implement our third requirement:\n",
        "\n",
        "3. Does your friend have any recipes that use this substitute?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvpOZAiX1WRx"
      },
      "source": [
        "But first, out of curiosity, let's see what it would look like if we were to ask the LLM to generate a recipe for us.\n",
        "\n",
        "This consists of adding a third and final chain to our SimpleSequentialChain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6uyo6OHHECz"
      },
      "outputs": [],
      "source": [
        "# Similar to before!\n",
        "vegan_recipe_generator_prompt = \"\"\"\n",
        "Give me a vegan recipe that uses this ingredient.\n",
        "\n",
        "Ingredient: {ingredient}\n",
        "Recipe: \"\"\"\n",
        "\n",
        "vegan_recipe_generation_prompt = PromptTemplate(\n",
        "    input_variables=[\"ingredient\"],\n",
        "    template=vegan_recipe_generator_prompt,\n",
        ")\n",
        "\n",
        "vegan_recipe_generator = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=vegan_recipe_generation_prompt,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eB-nYP71ms3"
      },
      "outputs": [],
      "source": [
        "vegan_recipe_generator_chain = SimpleSequentialChain(\n",
        "    chains=[\n",
        "        vegan_ingredient,\n",
        "        vegan_method,\n",
        "        vegan_recipe_generator,\n",
        "    ],\n",
        "    verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wNhlvWSHOgJ",
        "outputId": "92e0b6e7-1583-4504-c048-068dfb2522fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m  Plant-based milk (e.g. almond milk, coconut milk, oat milk, etc.)\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m Blend together 1 part plant-based milk (such as almonds, coconut, oats, etc.) with 3 parts water in a blender until smooth.\u001b[0m\n",
            "\u001b[38;5;200m\u001b[1;3m\n",
            "Vegan Milky Oatmeal\n",
            "Ingredients:\n",
            "\n",
            "• 2 cups plant-based milk (almonds, coconut, oats, etc.) \n",
            "• 6 cups water\n",
            "• 2 cups rolled oats\n",
            "• 2 tablespoons maple syrup or agave nectar\n",
            "• 1 teaspoon ground cinnamon\n",
            "• ½ teaspoon sea salt\n",
            "• 1 teaspoon vanilla extract\n",
            "• 2 tablespoons melted coconut oil\n",
            "• 1 cup mixed berries (fresh or frozen)\n",
            "• 2 tablespoons chopped nuts (optional)\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. In a blender, blend together the plant-based milk and the water until smooth. \n",
            "2. In a large mixing bowl, combine the milk-water mixture, oats, maple syrup or agave, cinnamon, salt, and vanilla extract and stir until combined.\n",
            "3. Heat the coconut oil in a non-stick skillet over medium-high heat.\n",
            "4. Pour the oat mixture into the skillet and reduce the heat to medium.\n",
            "5. Cook, stirring occasionally, for about 5 minutes until the oats are tender.\n",
            "6. Remove from heat and stir in the mixed berries and chopped nuts (if desired).\n",
            "7. Serve hot, topped with more berries and nuts if desired. Enjoy!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Vegan Milky Oatmeal\n",
            "Ingredients:\n",
            "\n",
            "• 2 cups plant-based milk (almonds, coconut, oats, etc.) \n",
            "• 6 cups water\n",
            "• 2 cups rolled oats\n",
            "• 2 tablespoons maple syrup or agave nectar\n",
            "• 1 teaspoon ground cinnamon\n",
            "• ½ teaspoon sea salt\n",
            "• 1 teaspoon vanilla extract\n",
            "• 2 tablespoons melted coconut oil\n",
            "• 1 cup mixed berries (fresh or frozen)\n",
            "• 2 tablespoons chopped nuts (optional)\n",
            "\n",
            "Instructions:\n",
            "\n",
            "1. In a blender, blend together the plant-based milk and the water until smooth. \n",
            "2. In a large mixing bowl, combine the milk-water mixture, oats, maple syrup or agave, cinnamon, salt, and vanilla extract and stir until combined.\n",
            "3. Heat the coconut oil in a non-stick skillet over medium-high heat.\n",
            "4. Pour the oat mixture into the skillet and reduce the heat to medium.\n",
            "5. Cook, stirring occasionally, for about 5 minutes until the oats are tender.\n",
            "6. Remove from heat and stir in the mixed berries and chopped nuts (if desired).\n",
            "7. Serve hot, topped with more berries and nuts if desired. Enjoy!\n"
          ]
        }
      ],
      "source": [
        "# Run the chain specifying only the input variable for the first chain.\n",
        "vegan_recipe = vegan_recipe_generator_chain.run(\n",
        "    \"milk\"\n",
        "    )\n",
        "print(vegan_recipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQCl6jAs146O"
      },
      "source": [
        "Great! What's cool about this is if you chain everything together you can see the intermediate results which could be useful for your application, for if you need to debug for example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_-OdKIsKw1k"
      },
      "source": [
        "Now, of course you could directly just make a similar call through a prompt to an LLM, like the below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMcf2xdqFlRf",
        "outputId": "1db4aabb-2858-4c83-9b8f-5769d2b8588e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Non-vegan ingredient: Milk\n",
            "Best vegan version: Plant-based milk (e.g. almond, oat, soy, coconut, etc.)\n",
            "Instructions on how to make it: \n",
            "1. Choose the type of plant-based milk you want to make.\n",
            "2. Place one cup of raw nuts/seeds or rolled oats in a blender.\n",
            "3. Add four cups of filtered water to the blender.\n",
            "4. Blend the mixture on high for two minutes.\n",
            "5. Strain the mixture through a cheesecloth or nut milk bag.\n",
            "6. Store in the refrigerator for up to five days.\n",
            "\n",
            "Vegan recipe using plant-based milk: Vegan Chocolate Chip Pancakes \n",
            "Ingredients: \n",
            "- 1 cup plant-based milk of choice \n",
            "- 2 tablespoons of apple cider vinegar \n",
            "- 2 cups of all-purpose flour \n",
            "- 2 tablespoons of baking powder \n",
            "- 1 teaspoon of salt \n",
            "- 2 teaspoons of sugar \n",
            "- 2 tablespoons of melted vegan butter \n",
            "- 1 cup of vegan chocolate chips \n",
            "Instructions:\n",
            "1. In a medium-sized bowl, combine plant-based milk and apple cider vinegar. Let the mixture sit for 5 to 10 minutes\n"
          ]
        }
      ],
      "source": [
        "template = \"\"\"\n",
        "Convert the non-vegan ingredient to the single best vegan version.\n",
        "Use whole foods.\n",
        "Then give me the instructions on how to make it.\n",
        "Then give me a vegan recipe that uses the vegan version of this ingredient.\n",
        "Description: {ingredient}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "vegan_recipe_prompt_one_shot = PromptTemplate(\n",
        "    input_variables=[\"ingredient\"],\n",
        "    template=template,\n",
        ")\n",
        "\n",
        "description = \"milk\"\n",
        "\n",
        "vegan_recipe_chain_one_shot = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=vegan_recipe_prompt_one_shot,\n",
        ")\n",
        "print(vegan_recipe_chain_one_shot.run(description))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESvafNTKGM7-"
      },
      "source": [
        "So you can see we can actually generate a recipe by using a single prompt. Why would we use a chain?\n",
        "\n",
        "Well, remember, we want to actually browse through our friend's recipes in an efficient way. We don't want to use an LLM for ALL of it -- we want to stop part way through and search for recipes in our friend's messy text file to see if anything good exists.\n",
        "\n",
        "So let's look at how we can connect to a dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnhQNuT_K4OK"
      },
      "source": [
        "# Connect to a database\n",
        "<!--\n",
        "Our steps will be as follows:\n",
        "1. get the vegan version of an ingredient\n",
        "2. get a method for how to make that ingredient\n",
        "3. get a recipe using that ingredient. -->\n",
        "\n",
        "\n",
        "Let's remind ourselves of our goal. We want to learn about plant based eating, and we started at the ingredient level: *Given an ingredient, what is the best plant based substitute?*\n",
        "\n",
        "We learned how to make that ingredient at home: *Given that best plant based substitute, how can you actually make it at home?*\n",
        "\n",
        "\n",
        "And then we grabbed a random recipe generated from an LLM using that ingredient.\n",
        "\n",
        "But we want to look through our friend's ingredients! *Does your friend have any recipes that use this substitute?*\n",
        "\n",
        "But, if your friend doesn't have a recipe that contains that ingredient, then let's generate something from the LLM.\n",
        "\n",
        "So our flow will be as follows:\n",
        "\n",
        "1. Get the vegan version of an ingredient (using vegan_ingredient chain)\n",
        "2. Get a method for how to make that ingredient (using vegan method chain, or vegan_ingredient_and_method_chain to chain steps 1 and 2 together).\n",
        "3. Get your friend's recipe using that vegan ingredient, but if your friend doesn't have a recipe with that ingredient, then generate a new recipe.\n",
        "\n",
        "We will make queries to the LLM for steps 1 and 2, but for step 3, we'll query our own database using our friend's recipes, first. If no recipe shows up, then we'll make a call to the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_iFAFx4MJ2r"
      },
      "source": [
        "### Let's start by loading in our friend's recipes as a database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SCh8tAA23Vqx",
        "outputId": "52128b1f-001c-484f-b5ea-590cfbc7727c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.3.26-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.5.3)\n",
            "Collecting requests>=2.28 (from chromadb)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.7)\n",
            "Collecting hnswlib>=0.7 (from chromadb)\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb)\n",
            "  Downloading clickhouse_connect-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (938 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m938.7/938.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.1)\n",
            "Collecting fastapi>=0.85.1 (from chromadb)\n",
            "  Downloading fastapi-0.96.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.65.0)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.3.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.15)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.7.1)\n",
            "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb)\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect>=0.5.7->chromadb)\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.85.1->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.3.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (414 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.1/414.1 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (3.6.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (1.3.0)\n",
            "Building wheels for collected packages: hnswlib\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2119858 sha256=28533d209db09c22d580f99efa400e1327adc25f73ea475ea761a6a71abaf031\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "Successfully built hnswlib\n",
            "Installing collected packages: tokenizers, monotonic, zstandard, websockets, uvloop, requests, python-dotenv, pulsar-client, overrides, lz4, humanfriendly, httptools, hnswlib, h11, backoff, watchfiles, uvicorn, starlette, posthog, coloredlogs, clickhouse-connect, onnxruntime, fastapi, chromadb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 chromadb-0.3.26 clickhouse-connect-0.6.1 coloredlogs-15.0.1 fastapi-0.96.0 h11-0.14.0 hnswlib-0.7.0 httptools-0.5.0 humanfriendly-10.0 lz4-4.3.2 monotonic-1.6 onnxruntime-1.15.0 overrides-7.3.1 posthog-3.0.1 pulsar-client-3.2.0 python-dotenv-1.0.0 requests-2.31.0 starlette-0.27.0 tokenizers-0.13.3 uvicorn-0.22.0 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3 zstandard-0.21.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "! pip install chromadb\n",
        "# ! pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haoKSyYejfN-",
        "outputId": "cf22a374-ad61-480e-962c-fc32998b97e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8QGPY3Ju9GL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "AmC9O1RR3aur",
        "outputId": "b0dd763b-24f8-48b5-fbfe-bf43a35b3418"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0df2d7e9-1410-41c5-a652-be5abb2386e9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0df2d7e9-1410-41c5-a652-be5abb2386e9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving vegan_recipes.txt to vegan_recipes.txt\n"
          ]
        }
      ],
      "source": [
        "# Upload our friend's recipes\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mybg_R0JMWi9"
      },
      "source": [
        "Great, now that we've loaded our friend's recipes into this notebook, we can get back to LangChain!\n",
        "\n",
        "We'll load our friend's recipes into LangChain by using a TextLoader.\n",
        "\n",
        "Once we load the recipes into LangChain, we will add them to a Vector Store.\n",
        "\n",
        "Essentially a vector store is a database, that is optimized for storing documents, and their embeddings, or mathematical representations. Vector stores work well for cases where you would like to input a query, and then output the document that is the most similar to the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UFKqvS-3ZVJ"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "\n",
        "# load into LangChain and create a vector store\n",
        "vegan_recipe_loader = TextLoader('vegan_recipes.txt')\n",
        "# index the recipes\n",
        "vegan_recipe_index = VectorstoreIndexCreator().from_loaders([\n",
        "    vegan_recipe_loader\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyyKOYkR3yRy"
      },
      "source": [
        "Now that our friend's recipes have been indexed (embedded) we can talk to the database!\n",
        "\n",
        "So we will ask a question, which will get embedded into some representation by LangChain. Then under the hood, LangChain searches the vector store for the document with the most similar embedding to the query. Then it outputs the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXPfWpk_3x8_",
        "outputId": "6e5d7df3-cb39-423e-e8c0-40ccb4125c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a question: Which recipes do not use almond milk?\n",
            " The base and the cream layer recipes do not use almond milk.\n"
          ]
        }
      ],
      "source": [
        "question = input('Ask a question: ')\n",
        "answer = vegan_recipe_index.query(question)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucK9dBi_NIqT"
      },
      "source": [
        "Great, so now we know we can \"talk\" to the database, using natural language! Now we can get back to creating an application of sorts that does the following:\n",
        "\n",
        "1. Get the vegan version of an ingredient\n",
        "2. Get a method for how to make that ingredient\n",
        "3. Get a friend's recipe for that ingredient, but if you friend doesn't have a recipe with that ingredient, then generate a new recipe.\n",
        "\n",
        "Steps 1 and 2 we already did previously. Let's remind ourselves of what we have already done by looking at the chains themselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1CwWZshy1F_",
        "outputId": "17a4cc71-8849-4e23-d905-ac1a9dbfb754"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, prompt=PromptTemplate(input_variables=['ingredient'], output_parser=None, partial_variables={}, template=\"\\n  Convert the non-vegan ingredient to the single best vegan version.\\n  Don't tell me how to make it. Just tell me the ingredient.\\n  Use whole foods.\\n  Don't give me multiple responses. Just one ingredient will do.\\n\\n  Ingredient: {ingredient}\\n\\n  Answer:\\n\", template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.9, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all'), output_key='text')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# Recall the first step: vegan_ingredient chain\n",
        "vegan_ingredient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ruw9IhQz9qJ",
        "outputId": "66c9670b-131e-4787-fb6b-9caa54c57566"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, prompt=PromptTemplate(input_variables=['ingredient'], output_parser=None, partial_variables={}, template=\"How do I make this ingredient?\\n\\nDon't create a list. Just output the instructions in a single sentence.\\n\\nExample 1:\\n- Ingredient: 'tofu steak'\\n- The question you understand this as is: 'how do I make tofu steak'?\\n- Instructions: 'Marinade extra firm tofu for 15 minutes in oil and spices and then grill for 10 minutes'.\\n\\nExample 2:\\n- Ingredient: 'tofu aioli'\\n- The question you understand this as is: 'how do I make tofu aioli'?\\n- Instructions: 'Blend together tofu, lemon juice, salt, and pepper.'\\n\\nIngredient: {ingredient}\\nInstructions: \", template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.9, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all'), output_key='text')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Our second standalone chain\n",
        "vegan_method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WEx_pZnO1LS"
      },
      "source": [
        "### Now let's see how we can combine vegan_ingredien and method_chain with a call to the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFn-q-nw2frS",
        "outputId": "c2eeaafa-66be-45a8-eabc-71b2e727fe71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your friend has a recipe that uses the vegan version of milk! The vegan ingredient becomes   Almond milk and the recipe is:\n",
            "\n",
            "Oatmeal:\n",
            "\n",
            "Ingredients:\n",
            "\n",
            "1 1/2 cups of oats\n",
            "1/4 cup of flaxseed meal\n",
            "1 cup of almond milk\n",
            "3/4 cup of water\n",
            "2 bananas\n",
            "1 tablespoon maple syrup\n",
            "3 tablespoons peanut butter\n",
            "\n",
            "Directions:\n",
            "\n",
            "1. In a medium saucepan, combine the oats, flaxseed meal, almond milk, and water.\n",
            "\n",
            "2. Bring to a boil over medium-high heat, stirring occasionally.\n",
            "\n",
            "3. Reduce heat to low and simmer for 5 minutes, stirring occasionally.\n",
            "\n",
            "4. Add the bananas, maple syrup, and peanut butter.\n",
            "\n",
            "5. Simmer for an additional 5 minutes, stirring occasionally.\n",
            "\n",
            "6. Serve warm and enjoy!\n",
            "And here is how you make   Almond milk at home:\n",
            " Blend raw almonds and water together until a smooth consistency is achieved.\n"
          ]
        }
      ],
      "source": [
        "INPUT_INGREDIENT = 'milk'\n",
        "\n",
        "# 1. Get the vegan version of an ingredient using vegan_chain\n",
        "vegan_ingredient_output = vegan_ingredient.run(INPUT_INGREDIENT)\n",
        "\n",
        "# 2. Get a method for it using vegan_method\n",
        "vegan_method_output = vegan_method.run(vegan_ingredient_output)\n",
        "\n",
        "# 3. At the same time, we'll search our friend's database for a recipe using\n",
        "# that vegan ingredient, now that we know how to make it from scratch!\n",
        "database_search_prompt = f\"\"\"\n",
        "Find a recipe in the database that contains {vegan_ingredient_output} as an ingredient.\n",
        "Output the recipe.\n",
        "If there is no such recipe, output 'Sorry! Your friend doesn't have a recipe using {vegan_ingredient_output}'.\n",
        "\"\"\"\n",
        "\n",
        "# We'll query the index, which is like a database\n",
        "recipe = vegan_recipe_index.query(database_search_prompt)\n",
        "\n",
        "# here's how we handle generating a new recipe, if your friend doesn't have\n",
        "# a recipe that uses the vegan version of that query ingredient.\n",
        "if 'Sorry!' not in recipe:\n",
        "  print(f'Your friend has a recipe that uses the vegan version of {INPUT_INGREDIENT}! The vegan ingredient becomes {vegan_ingredient_output}, and the recipe is:')\n",
        "  print(recipe)\n",
        "  print(f'And here is how you make {vegan_ingredient_output} at home:')\n",
        "  print(vegan_method_output)\n",
        "\n",
        "if 'Sorry!' in recipe:\n",
        "  print(f'Your friend does not have a recipe that uses the vegan version of {INPUT_INGREDIENT}! Let\\'s generate one!')\n",
        "\n",
        "  # llm = OpenAI(temperature=0.9)\n",
        "\n",
        "  template = \"\"\"Give me a vegan recipe that uses {vegan_ingredient_output} as an ingredient.\n",
        "  Recipe: \"\"\"\n",
        "\n",
        "  prompt = PromptTemplate(\n",
        "      input_variables=[\"vegan_ingredient_output\"],\n",
        "      template=template,\n",
        "  )\n",
        "  chain = LLMChain(\n",
        "      llm=llm,\n",
        "      prompt=prompt,\n",
        "    )\n",
        "  print(f'A recipe with the vegan version of {INPUT_INGREDIENT} does not exist! The vegan ingredient becomes {vegan_ingredient_output}. A new recipe is:')\n",
        "  recipe = chain.run(vegan_ingredient_output)\n",
        "  print('Here is our generated recipe:')\n",
        "  print(recipe)\n",
        "  print()\n",
        "  print(f'And here is how you make {vegan_ingredient_output} at home:')\n",
        "  print(vegan_method_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN1LroWq4ijn"
      },
      "source": [
        "Anyways, this probably isn't the most efficient but hey at least now you can see how and why you might want to use LangChain to search over a document!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_8DxMVUodZy"
      },
      "source": [
        "# Building a Q&A Application\n",
        "\n",
        "Great! Now we're able to talk to your friend's recipe database, just like your friend was here :')\n",
        "\n",
        "We just developed a way for us to ask single questions to the database through querying it. But we aren't able to have a conversation. A conversation requires memory!\n",
        "\n",
        "Note that the entry point in the flow we previously built was to input an ingredient, and we used Chains to obtain an output recipe.\n",
        "\n",
        "But what if we want to just directly \"chat\" with your friend's database, without having to specify an input ingredient? What if we want to talk to it, using natural language?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuj-qR8MVwQr"
      },
      "source": [
        "## First, let's recall... we've actually done this already!\n",
        "\n",
        "This is one such way to do it. LangChain has many possibilities! Another way would be using a `question_answering` chain."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = input('Ask a question: ')\n",
        "answer = vegan_recipe_index.query(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfcmDl1Ylv-K",
        "outputId": "676d4f8e-d87a-4bf0-d899-12539bf8e1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a question: Which of our friend's recipes contain almond milk?\n",
            " The oatmeal recipe contains almond milk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7uTltOU-3ly"
      },
      "outputs": [],
      "source": [
        "# ! pip install unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5XElgQ0_H06"
      },
      "outputs": [],
      "source": [
        "# ! pip install pdf2image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Icuoc_lZDMz"
      },
      "source": [
        "Great, so now we asked a single question to the database. Now let's try to have a conversation! Will it work using our existing set up?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = input('Ask a question: ')\n",
        "answer = vegan_recipe_index.query(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX9lFwRZl99U",
        "outputId": "2ac2b5b3-b859-4eaf-e27e-925142df323b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask a question: What question did I just ask you? I forget.\n",
            " You asked what ingredients are needed for the Roasted Brussels Sprouts recipe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jPTAs-9ZMAn"
      },
      "source": [
        "As you can see, no, this won't work. We need to try something else.\n",
        "\n",
        "## Let's use a ConversationalRetrievalChain. This will let us actually incorporate memory, which is important in order to have a conversation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvP3AHUXocXh"
      },
      "outputs": [],
      "source": [
        "# based off https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd4lOcilVPZ3"
      },
      "outputs": [],
      "source": [
        "# 1. Load in our recipes.\n",
        "recipes = vegan_recipe_loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our recipes are loaded, we will split them into chunks. Because remember, we input a single text document. That is long! We want to be able to search over it. We don't want to just straight up embed the entire document. Then we wouldn't be able to search within it!\n",
        "\n",
        "So we split our recipe doc into chunks, and then create embeddings of these chunks. This will let us search over every part of the recipe doc."
      ],
      "metadata": {
        "id": "ALM6GC9Hck6V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpMDRcNs_z1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "388dd6ec-14c6-49f0-cb2f-2056a7f12b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 1501, which is longer than the specified 1000\n"
          ]
        }
      ],
      "source": [
        "# 2. Split into characters.\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=0,\n",
        "    )\n",
        "recipes = text_splitter.split_documents(recipes)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create embeddings and add to an embedding store\n",
        "# we use Chroma because it's popular. *shrug*\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = Chroma.from_documents(\n",
        "    recipes,\n",
        "    embeddings,\n",
        ")"
      ],
      "metadata": {
        "id": "AWJNQhLCmXE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQISRl3p_zzU"
      },
      "outputs": [],
      "source": [
        "# 4. Add in memory! We'll use ConversationBufferMemory which is the most basic kind of memory.\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfj_wkBd_zw4"
      },
      "outputs": [],
      "source": [
        "# 5. Almost there... let's initialize our question answering LLM\n",
        "vegan_qa_llm = ConversationalRetrievalChain.from_llm(\n",
        "    OpenAI(temperature=0),\n",
        "    vectorstore.as_retriever(),\n",
        "    memory=memory,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaxIApc7aUbK"
      },
      "source": [
        "And with that, we can chat with our friend's recipes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKOxWYdn_zud",
        "outputId": "5d358870-ac80-4eb9-fec7-4f5d7bfecb4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The recipes for Tofu Pesto Aioli and Braised Tofu with Vegetables both contain tofu as an ingredient.\n"
          ]
        }
      ],
      "source": [
        "query = \"Which recipes in my friend's database contains tofu as an ingredient?\"\n",
        "result = vegan_qa_llm({\"question\": query})\n",
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf8mARSPAMYq",
        "outputId": "4859baee-1919-413a-e052-49872fcce44a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I don't know.\n"
          ]
        }
      ],
      "source": [
        "query = \"Which recipes in my friend's database contains almond milk as an ingredient?\"\n",
        "result = vegan_qa_llm({\"question\": query})\n",
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuAi5Ue-AQut",
        "outputId": "c943b0aa-6caa-43b0-dbd8-246e5ee96d09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The two recipes that contain tofu as an ingredient are Tofu Pesto Aioli and Roasted Brussels Sprouts.\n"
          ]
        }
      ],
      "source": [
        "query = \"What was the first answer you gave me? Please remind me.\"\n",
        "result = vegan_qa_llm({\"question\": query})\n",
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQC7skYpooeI"
      },
      "outputs": [],
      "source": [
        "# total time: about 35 minutes"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}